{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the next fitting technique. Wooo!!!!! So, principal component analysis (PCA) seems like it will be the most promising fitting method, as evidenced in the paper (de Oliveira-Costa et al. 2008). It is an efficient way to compress data as we are able to fit the data with as few parameters as possible while maintaining accuracy. When there are too many parameters, it leads to the risk of overfitting. \n",
    "\n",
    "Summary of steps for PCA:\n",
    "<ol>\n",
    "<li>Standardize the data</li>\n",
    "<li>Find the covariance matrix </li>\n",
    "<li>Compute eigenvalues and eigenvectors </li>\n",
    "<li>Rank eigenvectors </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args path to data and frequency\n",
    "#returns data matrix and data dictionary\n",
    "def data_matrix(path, freq):\n",
    "    data = np.load('Data/'+path)\n",
    "    d_matrix = data['arr_0'] #data matrix\n",
    "    #for the y values iterate through number of pixels and get the column of intensity \n",
    "    #column of intensity corresponds to all intensity values of that certain freq\n",
    "    data_dict = dict(zip(freq,(d_matrix[:,i] for i in range(d_matrix.shape[0]))))#x points keys and ypoints  values\n",
    "    return d_matrix, data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization and Finding Covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we need to calculate the covariance matrix. The covariance matrix should be the same dimensions as the dimensions for data which, in this case, is 2 dimensions. There are different ways to calculate the covariance matrix but essentially it requires normalizing the data set by subtracting off the mean. The steps used here to calculate the covariance matrix is as follows:\n",
    "<br>\n",
    "###  Standardize data set\n",
    "To standardize it, I just subtracted the mean value from the data set to make sure the data is the same scale. \n",
    "    Let $X$ be the matrix of the data of $nxp$ dimensions such that $X = \\begin{bmatrix}\n",
    "            x_{11} & \\ldots & x_{1p} \\\\\n",
    "            \\vdots &  & \\vdots \\\\\n",
    "            x_{n1} & \\ldots & x_{np}\\\\\n",
    "            \\end{bmatrix} $\n",
    "    <ol>\n",
    "      <li>Found the mean of each column of the matrix then put it in a vector. Let j = 1,..,p. \n",
    "                 $$u_{j} = \\frac{1}{n} \\sum_{i=1}^{n}{X_{ij}} \\quad where \\quad \\bar{u} = \\begin{bmatrix}\n",
    "                                                                                u_{1} \\\\\n",
    "                                                                                \\vdots \\\\\n",
    "                                                                                u_{p} \\\\\n",
    "                                                                                \\end{bmatrix}$$</li>\n",
    "            <li>Multiplied a vector of ones (h vector of $nx1$ size) and the transpose mean vector ($1xp$) to create a $nxp$ matrix of the mean values.</li>\n",
    "                 $$\\bar{h}\\bar{u}^{T}$$\n",
    "            <li>Subtracted mean matrix ($nxp$) from the $nxp$ matrix of data set to form B matrix ($nxp$).</li>\n",
    "                  $$B = X - \\bar{h}\\bar{u}^{T}$$\n",
    "</ol>\n",
    "\n",
    "### Find the covariance matrix\n",
    " With the standardized matrix found (B matrix), the covariance matrix of $pxp$ size can be calculated with\n",
    "    $$C = \\frac{1}{n-1} B^{T}B$$\n",
    "\n",
    "Credit: I based this off the wiki page for PCA (https://en.wikipedia.org/wiki/Principal_component_analysis#Derivation_of_PCA_using_the_covariance_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args matrix of data\n",
    "#returns covariance of data and normalized data set\n",
    "def cov_matrix(x_matrix):\n",
    "    column_vec = x_matrix[:,np.arange(x_matrix.shape[1])] #taking each column vector of matrix\n",
    "    mean_vector = np.c_[np.mean(column_vec,axis=0)] #calculating mean for each column and adding to vector\n",
    "    ones_vector = np.ones([x_matrix.shape[0],1]) #one vector \n",
    "#     print('Mean value ', np.dot(ones_vector,mean_vector.T), 'and mean vector \\n', mean_vector)\n",
    "    b_matrix = x_matrix - np.dot(ones_vector,mean_vector.T) #subtracting mean value \n",
    "    cov = (np.dot(b_matrix.T,b_matrix))/(x_matrix.shape[0]-1) #covariance matrix formula\n",
    "    return cov,b_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix is defined by:\n",
    "$$R_{jk} = \\frac{C_{jk}}{\\sigma_{j} \\sigma_{k}}$$\n",
    "so that $-1 \\leq R_{jk} \\leq 1$ and $R_{jj} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arg matrix of data\n",
    "#returns correlation matrix\n",
    "def corr_matrix(x_matrix):\n",
    "    c, stand = cov_matrix(x_matrix) #finding covariance of data matrix\n",
    "    sigma = np.sqrt(np.diag(c)) #finding sigma vector from covariance \n",
    "    c /= sigma[:,None] #divide columns of c_matrix by sigma vector\n",
    "    c /= sigma[None,:] #divide rows of c_matrix by sigma vector\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the covariance matrix, we can find eigenvectors $\\bar{v}$ such that $C\\bar{v}=\\lambda \\bar{v}$ for eigenvalue $\\lambda$. For a $pxp$ covariance matrix there will be $p$ eigenvectors with a corresponding set of eigenvalues. To determine how much information or variance is attributed to each principal component, you can calculate the explained variance. You determine the sum of all the eigenvalues and divide each eigenvalue by that sum. The result is a percentage of the total variance that is explained by each eigenvalue.\n",
    "\n",
    "With the explained variance, we can rank the eigenvectors by the eigenvalue with their corresponding eigenvectors from highest to lowest to determine an order of significane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args matrix\n",
    "#returns eigenvalues, eigenvectors, and tuple of eigvec and eigvec\n",
    "def eig_values(c):\n",
    "    eigval,eigvec = np.linalg.eig(c) #finding eigenvalues and eigenvectors\n",
    "    eig_pairs = [(eigval[i],eigvec[:,i]) for i in range(eigvec.shape[1])] #creating a tuple of eigval and eigvec\n",
    "    eig_pairs.sort() #sorting from least to greatest\n",
    "    eig_pairs.reverse() #reversing order to greatest to least\n",
    "    return eigval, eigvec, eig_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#args eigenvalue\n",
    "#returns dictionary of ranked eigenvalues (keys number of rank and values explained variance of each eigenvalue)\n",
    "def ordered_eigval(e):\n",
    "    total_eig = np.sum(e)\n",
    "    var_exp = np.sort([(e[i]/total_eig) for i in np.arange(e.size)]) #calculating and sorting explained variance\n",
    "    var_exp = var_exp[::-1] #reverse array to descending order\n",
    "    e_dict = dict(zip(np.arange(1,var_exp.size+1),var_exp)) #adding ordered eigvalues to dictionary with rank as key\n",
    "    return e_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args title of graph, dictionary that holds ranked eigenvalues\n",
    "#returns graph of ranked eigenvalues\n",
    "def graph_eigval(title, eig_dict):\n",
    "    plt.figure()\n",
    "    plt.scatter(eig_dict.keys(),eig_dict.values())\n",
    "    plt.plot(list(eig_dict.keys()),list(eig_dict.values()))\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Principal component number')\n",
    "    plt.ylabel('Fraction of variance explained')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cubic Spline Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolation is the estimation of a value within a set of data points. The estimated curve passes through all the given points. Spline interpolation is a type of interpolation where the interpolant is a piecewise polynomial known as the spline. The cubic spline is the smoothest while also being the lowest degree. It also avoids the Runge's phenomenon where when interpolating with higher degrees can result in unexpected oscillations.\n",
    "\n",
    "Summary of steps for cubic spline interpolation for PCA:\n",
    "<ol>\n",
    "<li>Normalize data\n",
    "    <ul>\n",
    "    <li>Find rms: square each column of data (corresponding to each intensity values at that frequency), average those results, then take square root. The result is one rms value per frequency </li>\n",
    "        <li>Take data and divide it by corresponding rms value at every frequency</li> </ul></li>\n",
    "<li>Use normalized data to find principal component fits</li>\n",
    "<li>Interpolate rms values and eigenvectors with log frequency\n",
    "   <ul> <li>Note: frequency values subtituted in interpolation function to be uniform in log</li></ul></li>\n",
    "<li>Find fits by multiplying coefficients and interpolated eigenvectors</li>\n",
    "<li>Undo normalization by multiplying with interpolated rms</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args data matrix\n",
    "#returns rms for each frequency and normalized matrix\n",
    "def normalized_rms(self,matrix):\n",
    "    rms_freq = np.zeros(len(self.freq))\n",
    "    for i in np.arange(len(self.freq)):\n",
    "        freq_sqr = np.square(matrix[:,i]) #squaring each column of data matrix\n",
    "        rms_freq[i] = np.sqrt(np.mean(freq_sqr)) #taking square root of mean of each squared column and adding to rms_freq array\n",
    "    nor_matrix = matrix[:,None] /rms_freq #divides columns by rms_freq array\n",
    "    nor_matrix = matrix[None:,] /rms_freq #divides rows by rms_freq array\n",
    "    return rms_freq,nor_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args freq and intensity want to interpolate, new frequency values,coefficients, rms freq values\n",
    "#returns cubic interpolated fit\n",
    "def cub_interfit(self,inter_freq,inter_intensity,freq_new,coef,rms_freq):\n",
    "    rms_inter = CubicSpline(inter_freq,rms_freq) #interpolating rms with freq\n",
    "    inter = CubicSpline(inter_freq,inter_intensity,axis=0) #interpolated eigvec for 2 pc\n",
    "    inter = inter(freq_new) #substituting new freq\n",
    "    try: #multiplying coef and interpolated eigvec\n",
    "        fit = np.dot(coef.T,inter.T) \n",
    "    except:\n",
    "        fit = np.dot(coef.T,inter.reshape(1,inter.shape[0])) #inter 1d array need to reshape\n",
    "    fit = fit * rms_inter(freq_new)\n",
    "    return fit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
