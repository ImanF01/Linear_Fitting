{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are functions to determine the parameters ($\\hat{x}$) for the matrix equation $y^{model} = A\\hat{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forming matrix for data and y vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is being formed into data matrices which hold the data. Each matrix corresponds to a type of model: linear, quadratic, cubic, or power law (below).\n",
    "\n",
    "$$A_{linear} = \\begin{bmatrix}\n",
    "1 & x_{1} \\\\\n",
    "1 & x_{2} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\end{bmatrix}, \\quad\n",
    "A_{quadratic} = \\begin{bmatrix}\n",
    "1 & x_{1} & x_{1}^{2}\\\\\n",
    "1 & x_{2} & x_{2}^{2} \\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "\\end{bmatrix},\\quad\n",
    "A_{cubic} = \\begin{bmatrix}\n",
    "1 & x_{1} & x_{1}^{2} & x_{1}^{3}\\\\\n",
    "1 & x_{2} & x_{2}^{2} & x_{2}^{3} \\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "While the y vector is just the y points:\n",
    "$\\bar{y} = \\begin{bmatrix}\n",
    "y_{1} \\\\\n",
    "y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "\\end{bmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_matrix(self):\n",
    "    one_arr = np.ones([self.data.size,1]) #array of ones concatenated to matrices\n",
    "    lin = np.hstack([one_arr, self.data]) #appending array of ones to array of data matrix\n",
    "    return lin\n",
    "\n",
    "def quad_matrix(self):\n",
    "    lin = self.lin_matrix()\n",
    "    quad = np.hstack([lin,self.data**2]) #appending lin to array of data squared \n",
    "    return quad\n",
    "\n",
    "def cub_matrix(self):\n",
    "    quad = self.quad_matrix()\n",
    "    cub = np.hstack([quad,self.data**3]) ##appending quad to array of data cubed\n",
    "    return cub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an attempt to fit a power law in the form of $y = \\beta x^\\alpha$. To fit the power law, it must be linearized since, according to Adrian's notes, the \"linear part of the term 'linear fit' just means linear in the parameters\". One way to do that is by applying log to both sides to make:\n",
    "\n",
    "$log(y) = log (\\beta x^\\alpha) = log(\\beta) + log(x^\\alpha) = log(\\beta) + \\alpha log(x)$\n",
    "\n",
    "Therefore, the linearization of $y = \\beta x^\\alpha$ is $log(y) = log(\\beta) + \\alpha log(x)$. Let $y^{'}=log(y)$ and $x^{'}=log(x)$ so that $y^{'} = log(\\beta) + \\alpha x^{'}$.\n",
    "\n",
    "With this, we can pretty much proceed as with the linear case but here the x matrix and y vector will be:\n",
    "\n",
    "$$A_{power} = \\begin{bmatrix}\n",
    "1 & log(x_{1}) \\\\\n",
    "1 & log(x_{2}) \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\end{bmatrix}, \\quad\n",
    "\\bar{y} = \\begin{bmatrix}\n",
    "log(y_{1}) \\\\\n",
    "log(y_{2}) \\\\\n",
    "\\vdots \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pow_matrix(self):\n",
    "    one_arr = np.ones([self.data.size,1])\n",
    "    pow_m = np.hstack([one_arr,np.log(self.data)]) #appending array of ones to array of log xvalues\n",
    "    return pow_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise covariance matrix and y model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat{x}$ for $y^{model} = A\\hat{x}$ is defined as $\\hat{x} = [A^TN^{-1}A]^{-1}A^TN^{-1}\\bar{y}$. From the earlier functions, I already found $A$ and $\\bar{y}$. The $N$ matrix is defined as follows:\n",
    "$$ N = \\begin{pmatrix}\n",
    "\\sigma_{1}^{2} & 0 & 0 &\\ldots{} \\\\\n",
    "0 & \\sigma_{2}^{2} & 0 & \\ldots{} \\\\\n",
    "0 & 0 & \\sigma_{3}^{2} & \\ldots{} \\\\\n",
    "\\vdots & \\vdots & \\ddots \\\\\n",
    "\\end{pmatrix} $$\n",
    "where $\\sigma^{2}$ is the variance or standard deviation squared. As a default, the noise covariance is the identity matrix if a noise covariance isn't given. \n",
    "\n",
    "Since I have determined the values for $A, N, \\bar{y}$, finding $\\hat{x}$ is just a matter of multiplying everything for $\\hat{x} = [A^TN^{-1}A]^{-1}A^TN^{-1}\\bar{y}$. I broke down the steps of the process:\n",
    "<ol>\n",
    "<li>$A^TN^{-1}$</li>\n",
    "<li>$A^TN^{-1}\\bar{y}$</li>\n",
    "<li>$[A^TN^{-1}A]^{-1}$</li>\n",
    "<li>$\\hat{x} = [A^TN^{-1}A]^{-1}A^TN^{-1}\\bar{y}$</li>\n",
    "<li>$y^{model} = A\\hat{x}$</li>\n",
    "</ol>\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below finds the y values for the fit ($y^{model}$), parameters ($\\hat{x}$), and the error covariance which is defined as $V= [A^{T}N^{-1}A]^{-1}$. It follows the steps I outlined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters data_matrix, y points, and noise covariance\n",
    "#returns y_model, x_bar (parameters for fit) and error covariance\n",
    "def ymodel(self,data_matrix,yval,noise_cov=None):\n",
    "    if noise_cov == None:\n",
    "        noise_cov = np.identity(data_matrix.shape[0])\n",
    "\n",
    "    #calculating parameters\n",
    "    dot_matrix = np.dot(data_matrix.T,np.linalg.inv(noise_cov)) #Step 1\n",
    "    doty_matrix = np.dot(dot_matrix,yval) #Step 2\n",
    "    inv_matrix = np.linalg.inv(np.dot(dot_matrix,data_matrix)) #Step 3 (error covariance)\n",
    "    x_bar = np.dot(inv_matrix, doty_matrix) #Step 4\n",
    "    \n",
    "    #finding y model\n",
    "    predict_y = np.dot(data_matrix, x_bar) #Step 5\n",
    "    return predict_y, x_bar, inv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function extrapolates the y model past the given x points. You give the function a data matrix with additional points added to extend the curve. Then, it slices the matrix contain only the actual data (given by the index of it) to calculate the parameter ($\\hat{x}$). With this parameter it calculates the y values for the fit $(y^{model})$ using the extended data matrix that contains the additional points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#args extended data_matrix (with extra x points added), y points, index where actual data begins in data_matrix_extend\n",
    "#returns y_model, x_bar (parameters for fit) and error covariance                   \n",
    "def ymodel_extend(self,data_matrix_extend,yval,index_begin_x):\n",
    "    data_matrix = data_matrix_extend[index_begin_x:] #slicing array to section with only x values from data\n",
    "    predict_y,x_bar,inv_matrix = self.ymodel(data_matrix,yval)\n",
    "    predict_y = np.dot(data_matrix_extend, x_bar)\n",
    "    return predict_y,x_bar,inv_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to find error information on final parameters to ascertain how far the fit is to the true parameters. To determine it, use the error covariance defined as $V= [A^{T}N^{-1}A]^{-1}$. The square root of the diagonal of $V$ gives the error bar of each parameter. The off-diagonal elements tell us how the errors on different parameters are correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_bar(self,err_cov):\n",
    "    err = np.sqrt(np.diag(err_cov)) #taking the square root of the diagonal of V\n",
    "    return err"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
